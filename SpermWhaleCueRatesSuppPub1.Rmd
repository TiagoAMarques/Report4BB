
---
title: Code for producing results and figures of "Estimating sperm whale cue rates to inform passive acoustic density estimation via cue counting"
author: Marques, T. A. and Marques, C. S. and  Gkikopoulou, K. C.
date: \today
output:
  html_document:
    code_folding: hide
    fig_caption: yes
    force_captions: yes
    highlight: pygments
    number_sections: yes
    theme: cerulean
    df_print: paged
    toc: yes
    toc_depth: 5
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: 5
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(R.matlab)
library(knitr)
library(readxl)
library(tidyverse)
library(dplyr)
library(ggplot2)
library(ggpubr)
library("data.table")
library("readr")
library(mgcv)
library(mgcViz)
library(geepack)
library(lubridate)
library(Hmisc)
library(splines)
library(gganimate)
#for GLMMs
library(lme4)
library(nlme)
```

# Introduction

This document presents the code required to reproduce the figures and resulting in the manuscript "Estimating sperm whale cue rates to inform passive acoustic density estimation via cue counting", submitted to The Journal of the Acoustical Society of America, by **add all names here**.

The dataset considered here corresponds to summaries of numbers of regular echolocation clicks per deep dive cycle, for each of the sperm whale tags considered on the manuscript, namely the cue rates per deep dive cycle, corresponding to `ddata1`, the single object in `data_4_article_clickrates_deep_dive.rda`. Note that despite having been recorded at the deep dive cycle level, to be used in the manuscript “A sperm whale cautionary tale about estimating acoustic cue rates for deep divers”, submitted to The Journal of the Acoustical Society of America, by Marques, T. A., Marques, C. S. & and Gkikopoulou, K. C., the first step in the data pre-processing is to pool data for each tag record, as the tag is the fundamental (and more importantly independent) sampling unit. 

This data was created via an internal ACCURATE document from the data that corresponds to the times of detections for each echolocation click from the focal animal detected in each tag, and those times were obtained from the DTAG raw sound files as described in the methods section of the paper. For future reference, the processing of the objects considered here was done in an RMarkdown dynamic report entitled `Cue_Rates_For_Sperm_Whales.Rmd`. This document itself is not shared because the corresponding data required to process it is not public yet. We are planning on making that data (i.e. the times of each echolocation click from the tagged animal found on each of the tags) public after the publication of a separate paper in preparation about the estimation of cue rates for sperm whales, where factors affecting the estimated cue rates will be explored.

In the following we present the code required to reproduce the analysis in the paper, including code to reproduce all the figures and tables.

# DDC data reading 

We begin by reading the deep dive cycle data in:

```{r}
# file created in Cue_Rates_For_Sperm_Whales.Rmd
# Reading the data that contain the information per deep dive cycle - object ddata1
load("data_4_article_clickrates_deep_dive.rda")
#removing the tags for animals we know were exposed to sonar
DDCs<-ddata1[ddata1$sonar!="sonar",]
```


# Initial data exploration and pre-processing

and from it, we aggregate data to create a per tag dataset,

```{r}
# Creating the data per tag
tags<-DDCs%>%
  group_by(tag)%>%
  summarise(location=unique(location), year=unique(year), sex=unique(sex),
  duration= sum(durations,na.rm=T),nclicks=sum(nclick,na.rm=T),
  crate=sum(nclick,na.rm=T)/sum(durations,na.rm=T),ddc=max(absdives+1,na.rm = T))
```

since the fundamental unit of analysis is the tag record.

We have a total of `r nrow(tags)` whales for which whales were not, knowingly, exposed to sonar. Additional tags were available but for which the tagged whales had been involved in controlled exposure experiments, and hence we discarded those from the analysis presented here. Understanding the effects of sonar exposure on whale behaviour, nad in particular cue rate production, is a separate research thread which requires information to be analysed at a much finer resolution, and for which context - for the large majority of the tags unavailable to us within ACCUARTE - would have to be incorporated. 


Tag recording duration ranged from `r round(min(tags$duration)/(60*60),3)` to `r round(max(tags$duration)/(60*60),3)` hours. The observed cue rates per tag varied between `r round(min(tags$crate),3)` and `r round(max(tags$crate),3)`, with a mean value of `r round(mean(tags$crate),3)` and  median value of `r round(median(tags$crate),3)`.  The individual cue rates per tag record, pooled across years and locations, are shown in the following figure:

```{r}
ggplot(tags,aes(x=1,y=crate),fill="lightblue")+
  theme_bw()+geom_violin(fill="lightblue")+
  geom_jitter()+ylab("cue rate (clicks per second)")+xlab("")
```


Below we present a table with the locations and years covered by these tags:

```{r}
kable(table(tags$location,tags$year))
```

The number of tags per year-site combination varies considerably. Out of a total of `r length(unique(tags$location))` locations and `r length(unique(tags$year))`, and therefore `r length(unique(tags$location))*length(unique(tags$year))` year-location combinations, the majority (`r table(table(tags$location,tags$year))[1]` combinations) have no tags, with only `r sum(table(table(tags$location,tags$year))[-1])` having tags associated with. What might be the number of tag records required to obtain a reliable year-location cue rate estimate remains hard to evaluate, but several combinations are certainly below that minimum, namely for those with less than a handful of tags. The distribution of the number of tags per year-site combination for which we have tags is represented in the image below:

```{r}
#note the need to remove the first count
#corresponding to un-interesting year-site combinations
#with 0 tags
barplot(table(table(tags$location,tags$year))[-1])
```

And we can take a look at the cue rates (pooled across locations) per year

```{r}
with(tags,boxplot(crate~year,ylab="Cue rate (clicks/sec)",xlab="",cex.lab=0.8,cex.axis=0.8))
```

and those (pooled across years) per location

```{r}
par(mar=c(8,4,0.2,0.2))
with(tags,boxplot(crate~location,ylab="Cue rate (clicks/sec)",xlab="",cex.lab=0.8,cex.axis=0.8))
```


We are hoping to explain variability in cue rates as a function of year and location. To do so, year-location combinations with a single tag are associated with an effect that is hard (strictly impossible, under a model with an interaction term) to estimate. We therefore removed years and locations for which only a single tag existed, effectively removing the single tag from Norway Andenes, also the only tag from 2005. Additionally, we grouped  tags from single tag year-location combinations into an adjacent year for the same same location. Hence, the single

* tag from Norway in 2009 was added to the 3 tags from the year 2010;
* tag from Dominica in 2017  was added to the remaining 4 tags from 2016;
* tag from Mediterranean in 2001 was added to the 7 tags from the year 2003.

```{r}
#removing the site with a single tag
tags<-tags[tags$location!="Norway Andenes",]
#grouping single tag per year into adjacent years
tags$year[tags$location=="Norway" & tags$year==2009]<- 2010
tags$year[tags$location=="DOMINICA" & tags$year==2017]<- 2016
tags$year[tags$location=="Mediterranean" & tags$year==2001]<- 2003
```

The above plots become:

```{r}
par(mfrow=c(2,1),mar=c(4,4,0.2,0.2))
with(tags,boxplot(crate~year,ylab="Cue rate (clicks/sec)",xlab="",cex.lab=0.8,cex.axis=0.8))
par(mar=c(8,4,0.2,0.2))
with(tags,boxplot(crate~location,las=2,ylab="Cue rate (clicks/sec)",xlab="",cex.lab=0.8,cex.axis=0.8))
```

# Estimating cue rates

Our objective is to obtain a cue rate estimate, and its desired precision to include in a cue counting density estimator. 

We can distinguish different levels of difficulty in terms of cue rate estimation with regards to the available information to do so. From easiest to hardest, we might want to estimate a cue rate for:

* a year and location we have data for
* a location we have data for at a year we do not have data for
* a year we have data for at a location we do not have data for
* a completely new year and location combination

From a conceptual point of view, if we had enough data across years and locations, one might want to:

1. treat as fixed effects those covariates for which we observed the level for which predictions are desired, but 
2. treat as random effects those covariates for which the level at which we would like to predict were not observed

Ideally when considering a model with random effect(s) we would propagate into predictions the variability associated with predicting for a new, previously unobserved, level of the random effect.

From first principles, site might be a sensible fixed effects covariate, since different sites will present different depths and prey distributions, and hence foraging at different depths might occur, and consequently different cue rates per site (across years), it seems like the variability from year might be not driven by year itself, but as random fluctuations over time, perhaps more sensibly accounted for as a random effect. If one believes this to be the case, to predict cue rates for:

1. a given site and year one could consider to use a model with that site, propagating the variability of year as a random effect;
2. for a new site and year, one could would use a model with both site and year as random effects.

In other words, intuitively year seems more sensibly modelled as a random effect, while location might be more sensibly modelled as a fixed effect.

## Model fitting

We will assume that cue rate, a strictly positive quantitie, follow a Gamma distribution, and a log-link function will be considered within a generalized linear model (GLM) or generalized linear mixed model (GLMM) to ensure positive predictions.

### GLMs

We begin by looking at GLM models where both location and year are treated as fixed effects, first with year as a numerical covariate

```{r}
#run models
CRglm1<-glm(crate~location+year,data=tags,family=Gamma(link="log"))
#summary(CRglm1)
CRglm2<-glm(crate~location+year+year:location,data=tags,family=Gamma(link="log"))
#summary(CRglm2)
```

and then with year as a factor. The latter seems more sensible a priori.

```{r}
#making a new variable, year as factor
tags$fyear<-as.factor(tags$year)

CRglm3<-glm(crate~location+fyear,data=tags,family=Gamma(link="log"))
#summary(CRglm3)
CRglm4<-glm(crate~location+fyear+fyear:location,data=tags,family=Gamma(link="log"))
#summary(CRglm4)
```


According to AIC

```{r}
kable(AIC(CRglm1,CRglm2,CRglm3,CRglm4))
```


the best model considers both location and year as factors, while the interaction between these factors is not deemed relevant. Given the nature of year and expectations as noted above for temporal effects, using year as a factor seems indeed more sensible any way.


```{r}
#define indexes of year site combinations we are interested in
iGoM2002<-tags$location=="Gulf of Mexico" & tags$year==2002
iGoM2001<-tags$location=="Gulf of Mexico" & tags$year==2001
iDom2015<-tags$location=="DOMINICA" & tags$year==2015
iDom2018<-tags$location=="DOMINICA" & tags$year==2018
```

We illustrate how to estimate the cue rate, and its precision, for: 

* a location and year for which we have available data. We consider here the cases for which we have the most tags, namely 
    - the Gulf of Mexico in 2002 (`r sum(iGoM2002)` tags), and 
    - Dominica in 2015 (`r sum(iDom2015)` tags), and 
    
* for these same sites, two different years when we have less data, namely 
    - 2001 for the Gulf of Mexico (`r sum(iGoM2001)` tags) and 
    - 2018 for Dominica (`r sum(iDom2018)` tags).

```{r}
ntags.GoM2002 <- sum(iGoM2002)
ntags.GoM2001 <- sum(iGoM2001)
ntags.Dom2015 <- sum(iDom2015)
ntags.Dom2018 <- sum(iDom2018)
mean.cr.GoM2002 <- mean(tags$crate[iGoM2002])
mean.cr.GoM2001 <- mean(tags$crate[iGoM2001])
mean.cr.Dom2015 <- mean(tags$crate[iDom2015])
mean.cr.Dom2018 <- mean(tags$crate[iDom2018])
sd.cr.GoM2002 <- sd(tags$crate[iGoM2002])
sd.cr.GoM2001 <- sd(tags$crate[iGoM2001])
sd.cr.Dom2015 <- sd(tags$crate[iDom2015])
sd.cr.Dom2018 <- sd(tags$crate[iDom2018])
margin.cr.GoM2002 <- qt(0.975,ntags.GoM2002-1)*sd(tags$crate[iGoM2002])/sqrt(ntags.GoM2002)
margin.cr.GoM2001 <- qt(0.975,ntags.GoM2001-1)*sd(tags$crate[iGoM2001])/sqrt(ntags.GoM2001)
margin.cr.Dom2015 <- qt(0.975,ntags.Dom2015-1)*sd(tags$crate[iDom2015])/sqrt(ntags.Dom2015)
margin.cr.Dom2018 <- qt(0.975,ntags.Dom2018-1)*sd(tags$crate[iDom2018])/sqrt(ntags.Dom2018)
```

The point estimates and respective 95% confidence intervals using a standard average are:

* Gulf of Mexico in 2002 (`r sum(iGoM2002)` tags): `r round(mean.cr.GoM2002,2)` (`r round(mean.cr.GoM2002-margin.cr.GoM2002,2)`-`r round(mean.cr.GoM2002+margin.cr.GoM2002,2)`)
* Gulf of Mexico in 2001 (`r sum(iGoM2001)` tags): `r round(mean.cr.GoM2001,2)` (`r round(mean.cr.GoM2001-margin.cr.GoM2001,2)`-`r round(mean.cr.GoM2001+margin.cr.GoM2001,2)`)
* Dominica in 2015 (`r sum(iDom2015)` tags): `r round(mean.cr.Dom2015,2)` (`r round(mean.cr.Dom2015-margin.cr.Dom2015,2)`-`r round(mean.cr.Dom2015+margin.cr.Dom2015,2)`)
* Dominica in 2018 (`r sum(iDom2018)` tags): `r round(mean.cr.Dom2018,2)` (`r round(mean.cr.Dom2018-margin.cr.Dom2018,2)`-`r round(mean.cr.Dom2018+margin.cr.Dom2018,2)`)

Creating a summary table that simplifies visualizing the results:

```{r}
summary.crs<-data.frame(location=c("GoM","GoM","Dominica","Dominica"),
              year=c(2002, 2001, 2015,2018),
              cr=c(mean.cr.GoM2002,mean.cr.GoM2001,mean.cr.Dom2015,mean.cr.Dom2018),
              lci=c(mean.cr.GoM2002-margin.cr.GoM2002,mean.cr.GoM2001-margin.cr.GoM2001,
                    mean.cr.Dom2015-margin.cr.Dom2015,mean.cr.Dom2018-margin.cr.Dom2018),
              uci=c(mean.cr.GoM2002+margin.cr.GoM2002,mean.cr.GoM2001+margin.cr.GoM2001,
                    mean.cr.Dom2015+margin.cr.Dom2015,mean.cr.Dom2018+margin.cr.Dom2018))
```

The point estimates and respective 95% confidence intervals using the model with both year and location as fixed effects are easiest to obtain by recoding the year and location factors to have the desired levels of each factor as the intercept, and then just exponentiating back to the linear scale (for confidence intervals, we create these on the link scale and then transform back to the response scale the interval limits)

```{r}
#GoM, 2002
tags$fyear02<-factor(tags$fyear,levels=c(2002,2001,2003,2010,2013,2014,2015,2016,2018,2019))
tags$location<-factor(tags$location,levels=c("Gulf of Mexico","Azores","Kaikoura","Mediterranean","North Atlantic Delaware","Norway","DOMINICA"))
CRglm4GoM02<-glm(crate~location+fyear02,data=tags,family=Gamma(link="log"))
#summary(CRglm4GoM02)
#compute mean cue rate
glm.cr.GoM2002<-exp(summary(CRglm4GoM02)$coefficients[1,1])
glm.lci.cr.GoM2002<-exp(summary(CRglm4GoM02)$coefficients[1,1]-qt(0.975,summary(CRglm4GoM02)$df.residual)*summary(CRglm4GoM02)$coefficients[1,2])
glm.uci.cr.GoM2002<-exp(summary(CRglm4GoM02)$coefficients[1,1]+qt(0.975,summary(CRglm4GoM02)$df.residual)*summary(CRglm4GoM02)$coefficients[1,2])
summary.crs$crglm[1]<-glm.cr.GoM2002
summary.crs$lciglm[1]<-glm.lci.cr.GoM2002
summary.crs$uciglm[1]<-glm.uci.cr.GoM2002
#----
#GoM, 2001
tags$fyear01<-factor(tags$fyear,levels=c(2001,2002,2003,2010,2013,2014,2015,2016,2018,2019))
tags$location<-factor(tags$location,levels=c("Gulf of Mexico","Azores","Kaikoura","Mediterranean","North Atlantic Delaware","Norway","DOMINICA"))
CRglm4GoM01<-glm(crate~location+fyear01,data=tags,family=Gamma(link="log"))
#summary(CRglm4GoM01)
#compute mean cue rate
glm.cr.GoM2001<-exp(summary(CRglm4GoM01)$coefficients[1,1])
glm.lci.cr.GoM2001<-exp(summary(CRglm4GoM01)$coefficients[1,1]-qt(0.975,summary(CRglm4GoM01)$df.residual)*summary(CRglm4GoM01)$coefficients[1,2])
glm.uci.cr.GoM2001<-exp(summary(CRglm4GoM01)$coefficients[1,1]+qt(0.975,summary(CRglm4GoM01)$df.residual)*summary(CRglm4GoM01)$coefficients[1,2])
summary.crs$crglm[2]<-glm.cr.GoM2001
summary.crs$lciglm[2]<-glm.lci.cr.GoM2001
summary.crs$uciglm[2]<-glm.uci.cr.GoM2001
#----
#Dominica, 2015
tags$fyear15<-factor(tags$fyear,levels=c(2015,2002,2003,2010,2013,2014,2001,2016,2018,2019))
tags$location<-factor(tags$location,levels=c("DOMINICA","Gulf of Mexico","Azores","Kaikoura","Mediterranean","North Atlantic Delaware","Norway"))
CRglm4Dom15<-glm(crate~location+fyear15,data=tags,family=Gamma(link="log"))
#summary(CRglm4Dom01)
#compute mean cue rate
glm.cr.Dom2015<-exp(summary(CRglm4Dom15)$coefficients[1,1])
glm.lci.cr.Dom2015<-exp(summary(CRglm4Dom15)$coefficients[1,1]-qt(0.975,summary(CRglm4Dom15)$df.residual)*summary(CRglm4Dom15)$coefficients[1,2])
glm.uci.cr.Dom2015<-exp(summary(CRglm4Dom15)$coefficients[1,1]+qt(0.975,summary(CRglm4Dom15)$df.residual)*summary(CRglm4Dom15)$coefficients[1,2])
summary.crs$crglm[3]<-glm.cr.Dom2015
summary.crs$lciglm[3]<-glm.lci.cr.Dom2015
summary.crs$uciglm[3]<-glm.uci.cr.Dom2015
#----
#Dominica, 2018
tags$fyear18<-factor(tags$fyear,levels=c(2018,2001,2002,2003,2010,2013,2014,2015,2016,2019))
tags$location<-factor(tags$location,levels=c("DOMINICA","Gulf of Mexico","Azores","Kaikoura","Mediterranean","North Atlantic Delaware","Norway"))
CRglm4Dom18<-glm(crate~location+fyear18,data=tags,family=Gamma(link="log"))
#summary(CRglm4Dom18)
#compute mean cue rate
glm.cr.Dom2018<-exp(summary(CRglm4Dom18)$coefficients[1,1])
glm.lci.cr.Dom2018<-exp(summary(CRglm4Dom18)$coefficients[1,1]-qt(0.975,summary(CRglm4Dom18)$df.residual)*summary(CRglm4Dom18)$coefficients[1,2])
glm.uci.cr.Dom2018<-exp(summary(CRglm4Dom18)$coefficients[1,1]+qt(0.975,summary(CRglm4Dom18)$df.residual)*summary(CRglm4Dom18)$coefficients[1,2])
summary.crs$crglm[4]<-glm.cr.Dom2018
summary.crs$lciglm[4]<-glm.lci.cr.Dom2018
summary.crs$uciglm[4]<-glm.uci.cr.Dom2018
```


* Gulf of Mexico in 2002: `r round(glm.cr.GoM2002,2)` (`r round(glm.lci.cr.GoM2002,2)`-`r round(glm.uci.cr.GoM2002,2)`)
* Gulf of Mexico in 2001: `r round(glm.cr.GoM2001,2)` (`r round(glm.lci.cr.GoM2001,2)`-`r round(glm.uci.cr.GoM2001,2)`)
* Dominica in 2015: `r round(glm.cr.Dom2015,2)` (`r round(glm.lci.cr.Dom2015,2)`-`r round(glm.uci.cr.Dom2015,2)`)
* Dominica in 2018: `r round(glm.cr.Dom2018,2)` (`r round(glm.lci.cr.Dom2018,2)`-`r round(glm.uci.cr.Dom2018,2)`)

We compare below standard averages against values obtained from the glm

```{r}
#standard average
plot(x=(1:4)-0.2,y=summary.crs$cr,xaxt="n",ylim=c(0,2),xlim=c(0.5,4.5),xlab="",ylab="cue rate (clicks per second)")
with(summary.crs,segments(x0=(1:4)-0.2,x1=(1:4)-0.2,y0=uci,y1=lci))
axis(1, at=(1:4),summary.crs$location,line = 0)
axis(1, at=(1:4),summary.crs$year,line = 1,tick=FALSE)
#GLM
points(x=(1:4)+0.2,y=summary.crs$crglm)
with(summary.crs,segments(x0=(1:4)+0.2,x1=(1:4)+0.2,y0=uciglm,y1=lciglm))
```

What these four estimates illustrate we is that point estimates are not that different from each other, with almost complete overlap in confidence intervals, except for the GoM in 2002, with slightly higher estimated cue rate. It also illustrates how a cue rate estimate drawn from a reduced number of tags could result in inadmissible estimates (the naive 95% CI for the GoM in 2001 approaches 0, while negative values for the cue rate are not possible). From that perspective, estimates from the fitted model might be better, as these will avoid negative values by construction, induced by the log link. Interestingly, estimates from the GLM model are slightly less precise for all but the most variable year-site combination, the GoM in 2001, where the strength borrowed from all the tags analysis means the model based estimate is considerably more precise.

### GLMMs

Attempting a model with site as a fixed effect, but with year as a random effect, to predict cue rate for a site we have data from, in a year we do not have data at that site. Therefore we consider a Gamma log-link GLMM for cue rate. Let us consider we are predicting the cue rate for Dominica and the GoM in the year 2012.

```{r}
tags$location<-factor(tags$location,levels=c("DOMINICA","Gulf of Mexico","Azores","Kaikoura","Mediterranean","North Atlantic Delaware","Norway"))
crglmerDom<-glmer(crate~location+(1|fyear),data=tags,family=Gamma(link="log"))
tags$location<-factor(tags$location,levels=c("Gulf of Mexico","DOMINICA","Azores","Kaikoura","Mediterranean","North Atlantic Delaware","Norway"))
crglmerGoM<-glmer(crate~location+(1|fyear),data=tags,family=Gamma(link="log"))
#exp(summary(crglmerDom)$coefficients[1,1])
#exp(summary(crglmerGoM)$coefficients[1,1])
#lme4
```

The empirical mean cue rate across all years for the GoM is `r round(mean(tags$crate[tags$location=="Gulf of Mexico"]),2)` clicks per second and for Dominica is `r round(mean(tags$crate[tags$location=="DOMINICA"]),2)` clicks per second. On the other hand, considering the GLMM, those values are `r round(exp(summary(crglmerGoM)$coefficients[1,1]),2)` clicks per second and `r round(exp(summary(crglmerDom)$coefficients[1,1]),2)` clicks per second for the GoM and Dominica, respectively. So while the observed cue rate was larger in the GoM, it is estimated by the GLMM as being (ever if ever just) higher in Dominica.

This implies that most of the variability that the model with two fixed effects (location and year) was attributing to changes per location are accounted for by random fluctuations in variability per year in the GLMM, and the random effect just happened to be higher for years in which the GoM was sampled (in particular 2002).

# Predictions for new year-location combinations

The above results seem to suggest that it might be quite difficult to separate location and year effects.

A precautionary approach, when predicting a cue rate for new year and location combinations, might be to use both year and site as random effects in a random effects model and then propagate the uncertainty associated with the overall mean with respect to both random effects, year and site, that would be unobserved. This is implemented here:

```{r}
crglmer<-glmer(crate~(1|location)+(1|fyear),data=tags,family=Gamma(link="log"))
#crglmer<-glmer(crate~(1|as.factor(paste0(location+fyear)),data=tags,family=Gamma(link="log"))
```

Under this representation, the cue rate would be estimated, for any new site-year combination as `r round(exp(summary(crglmer)$coefficients[1,1]),2)` clicks per second. 

We note explicitly this is quite lower that the overall mean of all tags cue rates `r round(mean(tags$crate),2)` clicks per second, a consequence of the tags being considerably unbalanced across years and sites, with more tags in year-location combinations which happened to have higher cue rates. This is nonetheless closer to, as would be expected, the mean of the average cue rates per year-site combination (`r round(mean(tapply(X=tags$crate,INDEX = paste0(tags$location,tags$year),FUN=mean)),2)` clicks per second).

Note that the variability on the observed cue rates per year-location combination is not that large:

```{r}
crates.psycomb<-data.frame(cr=tapply(X=tags$crate,INDEX = paste0(tags$location,tags$year),FUN=mean),sy=tapply(X=tags$crate,INDEX = paste0(tags$location,tags$year),FUN=head,n=1))
ggplot(crates.psycomb,aes(x=1,y=cr),fill="lightblue")+
  theme_bw()+geom_violin(fill="lightblue")+
  geom_jitter()+ylab("cue rate (clicks per second)")+xlab("")
#boxplot(tapply(X=tags$crate,INDEX = paste0(tags$location,tags$year),FUN=mean))
```

To estimate the precision on this mean estimate for cue rate we would need to propagate the variability associated with the random effects into the point estimate of the intercept of the model. 

How can that be done? Bolker et al. note here under [Predictions and/or confidence (or prediction) intervals on predictions](https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html#predictions-andor-confidence-or-prediction-intervals-on-predictions) "none of the following approaches takes the uncertainty of the random effects parameters into account..." and suggest that "if you want to take RE parameter uncertainty into account, a Bayesian approach is probably the easiest way to do it.". 

Ben Bolker (BB) further provided several possible approaches as a reply to a question TAM posted on stack exchange:

https://stats.stackexchange.com/questions/616697/how-to-estimate-precision-on-a-prediction-for-a-glmm-for-an-unobserved-level-of/

BB describes a possible approach via an (intuitive) parametric bootstapp. The suggested idea would be to resample values from the distribution of the overall mean, and then add sampled values from the variability associated with each of the random effects, and compute the variability of the resulting estimates. These have been referred to in the literature as "population prediction intervals". We note in passing that BB notes that this bootstrap procedure might be hard to justify from a theoretical point of view:

https://stats.stackexchange.com/questions/590595/justification-for-population-prediction-intervals

The options would be:

1. a procedure involving the analytic estimates of the variances of the parameters
2. a quick (non-parametric) bootstrapp procedure
3. a full (nonparametric + parametric) bootstrapp procedure
4. a full Bayesian implementation

Here I attempt to implement the options 2 and 3.

In this case, the parametric bootstrapp  would be

```{r}
set.seed(123)
B<-9999
means<-rnorm(B,mean=summary(crglmer)$coefficients[1,1],sd=summary(crglmer)$coefficients[1,2])
desv.year<-rnorm(B,mean=0,sd=sqrt(as.numeric(summary(crglmer)$varcor[1])))
desv.loc<-rnorm(B,mean=0,sd=sqrt(as.numeric(summary(crglmer)$varcor[2])))
#estimates at a new year site combination, on the link scale
est.lmean.crs<-means+desv.year+desv.loc
#estimates on the scale of the response
est.mean.crs<-exp(est.lmean.crs)
```

and now to get a confidence interval we can simply use the percentile method, leading to a mean estimate of `r round(exp(summary(crglmer)$coefficients[1,1]),2)` and 95% confidence intervals of `r round(quantile(est.mean.crs,0.025),2)`-`r round(quantile(est.mean.crs,0.975),2)`.

Note that I did not implement the methods strictly as described by BB, instead I followed what I had thought about originally. BB suggested the following procedure:

* draw MVN samples (using `MASS::mvrnorm` or one of the other alternatives in the R ecosystem) from the distribution with the combined FE sampling variance + RE variance
* exponentiate them, and 
* draw Gamma samples based on those mean values
    
but it is unclear to me why, **and would love some additional insight from BB**, on why:

* one would want to implement the third step, draw from a Gamma, since what we are interested is in the variation on the mean of a new year-site combination, not the variance in observations from such a site;

* (this is potentially a mute point/detail, but nonetheless I would like to know) what said Gamma would be, given via `glmer` we only get the mean value of the Gamma. Where in the output of `glmer` lies the estimate for the shape parameter of the corresponding Gamma?

As per the answer by BB (see also section 5.3 in Bolker 2008 https://math.mcmaster.ca/~bolker/emdbook/chap7A.pdf), while the above procedure is sensible, this approach will ignore the variability associated with the estimation of the random effects, and a "full" bootstrap approach would first randomize independent sampling units (year-location combinations? individual tags? I lean towards the former, as that encompasses the second level of variability) observations, refit the model at each iteration, sampling from the variances associated with each random effect.

BUT... now I can get an estimate of the mean at each bootstrap iteration, but how do I get the variability on the random effects? At each iteration, there is none. Do I simply generate a single value for a possible mean of a year-site combination at each bootstrap resample? I guess so!

Therefore, in any case, I try to implement here the bootstrap which would account for the variability in estimating the random effects, by sampling year-location combinations

```{r,cache=TRUE,message=FALSE,warning=FALSE}
#define location-year combination as a variable
tags$ly<-with(tags,paste0(location,year))
#uniqye location-year combinations
lys<-unique(tags$ly)
#number of combinations
nlys<-length(lys)
B<-9999
res.boot<-numeric(B)
#for each bootstrapp resample
for(i in 1:B){
  #select a reasmple of location-year combinations
  ly1<-sample(lys,1)
  boot.tags<-tags[tags$ly==ly1,]
  for(j in 2:nlys){
    boot.tags<-rbind(boot.tags,tags[tags$ly==sample(lys,1),])
  }
  #fit the model
  #NOTE: while I silence the messages, we get lots of "boundary (singular) fit: see help('isSingular')" warnings
  crglmer.boot<-glmer(crate~(1|location)+(1|fyear),data=boot.tags,family=Gamma(link="log"))
  means.b<-rnorm(1,mean=summary(crglmer.boot)$coefficients[1,1],sd=summary(crglmer.boot)$coefficients[1,2])
  desv.year.b<-rnorm(1,mean=0,sd=sqrt(as.numeric(summary(crglmer.boot)$varcor[1])))
  desv.loc.b<-rnorm(1,mean=0,sd=sqrt(as.numeric(summary(crglmer.boot)$varcor[2])))
  #estimates at a new year site combination, on the link scale
  est.lmean.crs.b<-means.b+desv.year.b+desv.loc.b
  #estimates on the scale of the response
  res.boot[i]<-exp(est.lmean.crs.b)
}
```

and now to get a confidence interval we can again use the percentile method, leading to a 95% confidence interval of `r round(quantile(res.boot,0.025),2)`-`r round(quantile(res.boot,0.975),2)`. As expected, the 95% CI are, even if just ever so slightly, wider than when ignoring the component of variation due to the random effects estimation. We can see that overlaid with the point and interval estimates presented before:

```{r}
#standard average
plot(x=(1:4)-0.2,y=summary.crs$cr,xaxt="n",ylim=c(0,2),xlim=c(0.5,4.5),xlab="",ylab="cue rate (clicks per second)")
with(summary.crs,segments(x0=(1:4)-0.2,x1=(1:4)-0.2,y0=uci,y1=lci))
axis(1, at=(1:4),summary.crs$location,line = 0)
axis(1, at=(1:4),summary.crs$year,line = 1,tick=FALSE)
#GLM
points(x=(1:4)+0.2,y=summary.crs$crglm)
with(summary.crs,segments(x0=(1:4)+0.2,x1=(1:4)+0.2,y0=uciglm,y1=lciglm))
#the estimated mean cue rate
abline(h=exp(summary(crglmer)$coefficients[1,1]),lty=2)
#adding a new year site combination - 95% CI that ignore RE estimation variability
abline(h=quantile(est.mean.crs,probs=c(0.025,0.975)),col=3,lty=2)
#adding a new year site combination - 95% CI accounting for said variability
abline(h=quantile(res.boot,probs=c(0.025,0.975)),col=4,lty=2)
legend("topright",legend=c("Estimated cue rate","95% CI ignoring RE var","95% CI including RE var"),lty=2,col=c(1,3,4),inset=0.05)
```

# Aknowledgements

We thank Ben Bolker for helpful advice via the answer to our question on "Stack Exchange" at https://stats.stackexchange.com/questions/616697/how-to-estimate-precision-on-a-prediction-for-a-glmm-for-an-unobserved-level-of/